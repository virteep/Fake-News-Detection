# Evaluate Credibility of Web-Based News Articles by using NLP and Deep Learning
This project is part of my MS in Computer Science Capstone Project at Rochester Institute of Technology, NY. 

*This project is still under development*

## Proposal
The proliferation of fake news articles online reached a peak during the 2016 US Elections. Facebook, Twitter, and
other social media websites were rife with misleading content that aimed to demean the other candidates. Fraudulent news articles are rampant all over the internet and common examples include supposed cures of cancer and other diseases, biased
political and religious propaganda, clickbait articles, satirical articles, articles promoting conspiracy theories etc. Such articles are detrimental to ones general lifestyle and health. Following unverified political news may lead to the election
of a wrong candidate and believing in false health care tips may prove to be fatal. It creates doubt in peoples mind, leading
to unwise decisions and chaos. Fake news can include articles that contain falsified information, no connection between the
headline and the text, misleading content, biased opinions, false context or satires.

For the scope of this project, a reliable source of news is the one that does not contain falsified or biased information,
including satirical content. I will be building my own labeled dataset. A website called [OpenSources](http://www.opensources.co/) provides tags to news sources that do not have legitimate news articles. Types of tags include fake news, satire, extreme bias, hate news etc. I will be scraping the articles from these websites and they will constitute the ”fake news” section of the dataset. For the "real news" section, I will be scraping articles from non-biased sources mentioned in [MediaBiasFactCheck.com](https://mediabiasfactcheck.com/). I will be using natural language processing techniques, conventional machine learning models like Bayesian classifiers as well as deep learning models for the purpose of classification.

