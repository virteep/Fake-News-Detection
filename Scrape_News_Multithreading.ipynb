{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape News Articles from Websites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to scrape online news articles by using Newspaper library with multithreading. \n",
    "In this code, articles from websites publishing fake articles are collected.\n",
    "Link for fake news sources data - https://github.com/BigMcLargeHuge/opensources/tree/master/sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "News Scraping - https://github.com/ankkur13/Big-Data-Systems-and-Intelligence-Analytics/blob/master/News%20Data%20Scraping%20.ipynb\n",
    "Real News Sources - https://github.com/N2ITN/are-you-fake-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import csv\n",
    "from newspaper.article import ArticleException, ArticleDownloadState\n",
    "from time import sleep\n",
    "from newspaper import news_pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 15 #number of articles per source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = pd.read_csv('sources_fake1.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "companies.rename(columns={'Unnamed: 0': 'Source'}, inplace=True)\n",
    "companies.drop(['2nd type', '3rd type', 'Source Notes (things to know?)'], axis=1, inplace=True)\n",
    "companies['type'].fillna('fake', inplace=True)\n",
    "companies['type'] = companies['type'].str.strip()\n",
    "companies['Source'] = companies['Source'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping with Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = ThreadPool(10)   # no. of threads to use\n",
    "          \n",
    "\n",
    "def get_articles(company):\n",
    "    count=1\n",
    "    scraped_articles = pd.DataFrame(columns=['Source','SourceHTTP', 'Title', 'Authors', 'Text', 'URL','PublishedDate'])\n",
    "    \n",
    "    url = company.strip()\n",
    "    if 'http://' or 'https://' not in url:\n",
    "         _url = 'http://' + url\n",
    "\n",
    "    paper = newspaper.build(_url, memoize_articles=False, language='en')\n",
    "    \n",
    "    for content in paper.articles:\n",
    "        if count > LIMIT:\n",
    "            break\n",
    "        try:\n",
    "            slept = 0\n",
    "            content.download()\n",
    "            while content.download_state == ArticleDownloadState.NOT_STARTED or content.download_state != 2:\n",
    "                if slept > 20:\n",
    "                    raise ArticleException('Download never started')\n",
    "                sleep(1)\n",
    "                slept += 1\n",
    "            content.parse()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Continuing...\")\n",
    "            continue\n",
    "\n",
    "        if content.title:\n",
    "            if len(content.text) < 150:\n",
    "                continue\n",
    "            article = {}\n",
    "            article['Source'] = url\n",
    "            article['SourceHTTP'] = _url\n",
    "            article['Title'] = content.title\n",
    "            article['Authors'] = content.authors\n",
    "            article['Text'] = content.text\n",
    "            article['URL'] = content.url\n",
    "            \n",
    "            # If published date does not exist or is not in a recognizable format\n",
    "            if content.publish_date is None:\n",
    "                article['PublishedDate'] = None\n",
    "            else:\n",
    "                article['PublishedDate'] = pd.Timestamp(datetime.date(content.publish_date))\n",
    "                \n",
    "            scraped_articles = scraped_articles.append(article, ignore_index=True, sort=True)\n",
    "            \n",
    "            count = count + 1\n",
    "    count = 1\n",
    "    print(\"Done \", company)\n",
    "    return scraped_articles\n",
    "\n",
    "scraped_articles = pd.DataFrame(columns=['Source','SourceHTTP', 'Title', 'Authors', 'Text', 'URL','PublishedDate'])\n",
    "\n",
    "scraped_articles = scraped_articles.append(pool.map(get_articles, companies['Source']), ignore_index=True, sort=True)\n",
    "\n",
    "\n",
    "pool.close() \n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write back to a csv\n",
    "scraped_articles.to_csv('Fake_News_Articles.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
